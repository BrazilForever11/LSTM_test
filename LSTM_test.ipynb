{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us take simple example of sequence. \n",
    "# Assume we have 5 sequencies, each of length 3. Depended sequence\n",
    "# will compute average over all prior sequences, \n",
    "# round it to whole integer and will take last digit. \n",
    "# We do it so this is classifiction task as is illustrated in many places. \n",
    "# Here is example of a sequence:\n",
    "\n",
    "# x_1 = (1,2,3), y_1 = (1+2+3)/3 = 2\n",
    "# x_2 = (5,6,7), y_2 = 4\n",
    "# x_3 = (8,9,10), y_3 = 6\n",
    "# x_4 = (322,425,727), y_4 = 7\n",
    "# x_5 = (12, 12, 43), y_5 = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#constants\n",
    "VECTOR_SIZE = 3\n",
    "SEQUENCE_LENGTH = 5\n",
    "BATCH_SIZE = 7\n",
    "STATE_SIZE = 40\n",
    "NUM_CLASSES = 10\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(n = 210):\n",
    "    lower_bound = 0\n",
    "    upper_bound = 3000\n",
    "    x = [random.choice(range(lower_bound, upper_bound)) for _ in range(n)]\n",
    "    return x\n",
    "\n",
    "def y_from_x(x):\n",
    "    y = [round(np.mean(x[0:el,:].flatten()))%10 for el in range(1,len(x)+1)]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to generate whole epoch  i.e.  collection of batches.\n",
    "def get_epoch(n_batches = 100000):\n",
    "    raw_x = get_data(n = n_batches*SEQUENCE_LENGTH*VECTOR_SIZE*BATCH_SIZE)\n",
    "    reshaped_x =np.reshape(raw_x, (BATCH_SIZE, -1, VECTOR_SIZE))\n",
    "    split_x = np.split(reshaped_x, reshaped_x.shape[1]/SEQUENCE_LENGTH, axis = 1)\n",
    "    split_y = list()\n",
    "    for el in split_x:\n",
    "        split_y.append([y_from_x(el[sl,:,:]) for sl in range(len(el)) ])\n",
    "    split_y =[np.array(el) for el in split_y]\n",
    "    \n",
    "    for sample_x, sample_y in zip(split_x, split_y):\n",
    "        yield sample_x, sample_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN setup\n",
    "# we need placeholders for the input\n",
    "x = tf.placeholder(tf.float32, [BATCH_SIZE, SEQUENCE_LENGTH, VECTOR_SIZE],\n",
    "                   name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [BATCH_SIZE, SEQUENCE_LENGTH,],\n",
    "                   name='labels_placeholder')\n",
    "init_state = tf.zeros([BATCH_SIZE, STATE_SIZE])\n",
    "\n",
    "rnn_inputs = tf.unstack(x, axis = 1)\n",
    "y_as_list = tf.unstack(y, axis=1)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(STATE_SIZE, state_is_tuple = True)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs,\n",
    "                            initial_state=(init_state,init_state))\n",
    "\n",
    "# we need to process output further in order to get it in the form we need. \n",
    "# output of each cell has to be one hot vector of length 10. \n",
    "# 0 = [1,0,0,0,0,0,0,0,0,0], ...,\n",
    "# 9 =[0,0,0,0,0,0,0,0,0,1]. We should use softmax and let us use\n",
    "# cross entropy as loss function.\n",
    "\n",
    "# Let us define output \n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [STATE_SIZE, NUM_CLASSES])\n",
    "    b = tf.get_variable('b', [NUM_CLASSES], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Now let us deal with losses\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "# this requction is along sequnce length\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(LEARNING_RATE).minimize(total_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training(n_epoch = 10):\n",
    "    \n",
    "    epoch_loss_list = list()\n",
    "    for i, el in enumerate(range(n_epoch)):\n",
    "        epoch = get_epoch(n_batches=1000000)\n",
    "        epoch_loss = 0\n",
    "        for X,Y in epoch:\n",
    "            batch_total_loss = sess.run([total_loss, train_step],\n",
    "                                        feed_dict = {x:X,y:Y})\n",
    "            epoch_loss += np.sum(batch_total_loss[0])\n",
    "        epoch_loss_list.append(epoch_loss)\n",
    "    return epoch_loss_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_run = training(n_epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(n_batches = 1000):\n",
    "    epoch = get_epoch(n_batches= n_batches)\n",
    "    total_correct = 0\n",
    "    for X, Y in epoch:\n",
    "        predictions_result = sess.run([predictions], feed_dict = {x:X})\n",
    "        total_correct += count_correct(predictions_result[0], Y)\n",
    "    return total_correct/(n_batches*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014285714285714287"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us evaluate performance of our network. Let us simply count number \n",
    "# of correct sequences\n",
    "def count_correct(est_Y, true_Y):\n",
    "    # est_Y will get list of (batch_size x num_of_classes) array\n",
    "    # length of est_Y is length of our sequence.\n",
    "    # true_Y is of batch_size x sequence_length form.\n",
    "    # this method will return number of coinciding sequences.\n",
    "    est_Y = back_to_y(est_Y)\n",
    "    counter = 0\n",
    "    for el in range(BATCH_SIZE):\n",
    "        if np.array_equal(est_Y[el,:], true_Y[el,:]):\n",
    "            counter+=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need function to translate back to y \n",
    "def back_to_y(batch):\n",
    "    # output will be of the shape (batch_size x sequence_length)\n",
    "    # we are getting list with one element of list per our \n",
    "    # sequence element\n",
    "    result = np.zeros(shape=(7,5))\n",
    "    for i, el in enumerate(batch):\n",
    "        result[:,i] = np.argmax(el, axis =1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
